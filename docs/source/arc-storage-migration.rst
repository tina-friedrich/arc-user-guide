ARC Storage (Migration)
=======================


Introduction
------------

Over the last year the ARC team has been working on replacing part of the storage infrastructure behind the ARC and HTC clusters. This is necessary since the current storage infrastructure is end of life, and to meet the increasing space and performance demands. 

As a reminder - ``$DATA`` is the shared permanent storage available to projects to store research data required for (or generated by) use of the ARC clusters. ``$SCRATCH`` is the temporary storage created at the beginning of the job which can be used by programs to store temporary data shared by processes during a job run; ``$SCRATCH`` space is deleted after the job is stopped. There is a third area available to users, ``$HOME``, which (mostly) holds data required for the login process.

The new storage replaces ``$DATA`` and ``$SCRATCH`` data areas. Your ``$HOME`` area is unaffected.

More information on the different types of storage can be found on the ARC user guide at https://arc-user-guide.readthedocs.io/en/latest/arc-storage.html


Migration process
-----------------

Once you notify us that you are ready to start migration for your project, we will mount the new storage as a new "empty" ``$DATA`` area for your project, and your old (currrent) $DATA area read-only under ``/migration/$PROJECT/$USER``. This will allow you to - fairly quickly - copy the data you are currently working on and resume cluster activity, whilst continuing to transfer older data.

Each user is responsible for copying their own data from the old to the new location. If you are a member of multiple projects, you will need to move all your data areas separately.

Please contact ‘support@arc.ox.ac.uk’ if you would like your storage area switched, or discuss those options.

.. warning::
  Storage areas will be migrated on a *PER PROJECT* basis, if you request an empty ``$DATA`` area on the new storage, this MUST be with the agreement of all other members of your project. As the other users in your
  project will also get a new empty ``$DATA`` area (and this may affect any jobs they have running or queued)

How to transfer data
--------------------

There are many ways to copy files from one folder to another but not all are appropriate in this case. As mentioned above, using ``mv`` is not advisable as you do not want to remove the data from the old storage. ``cp`` is also not advisable as it is very slow. ``rsync`` is probably the best option, as it is interruptible and resumable, and can check data integrity - however, it may not be the fastest solution for very large trees. For very large trees, using a ``tar | tar`` solution is likely faster.

**rync commands**:

.. code-block:: shell

  cd /migration/<projectname>/$USER
  rsync –avhP . /data/<projectname>/$USER/

**tar commands**

.. code-block:: shell

  cd /migration/<projectname>/$USER
  tar cvf - . | tar xf - -C /data/<projectname>/$USER/ 

These can be run from an nx session, an interactive session, or submitted as a job on the cluster. An example submission script would look something like:

.. code-block:: bash

  #!/bin/bash 
  #SBATCH --nodes=1 
  #SBATCH --ntasks-per-node=1 
  #SBATCH --cpus-per-task=2 
  #SBATCH --partition=short 
  #SBATCH --job-name=Data_migration 
  
  module purge 

  # change the value of `MYPROJECT` to the project you want to migrate
  export MYPROJECT="engs-example"

  cd /migration/$MYPROJECT/$USER 
  rsync -avhP . /data/$MYPROJECT/$USER/

Be careful when using a cluster job, and especially when copying in an interactive session; the time limit might interrupt your transfer before it is complete.

It is of course also possible to only transfer certain sub-directories, or (especially using rsync) exclude certain subdirectories from the copy process. 
Please refer to the 'rsync' or 'tar' man pages for details, or ask the ARC team for assistance. 


Who is responsible for migrating my data?
-----------------------------------------

Each user is responsible for transferring their data; however, the project PI or a user appointed by the project PI is responsible for gathering progress from all project users. The switch from old to new has to be done on a project basis. We cannot move users individually.


Migration timeline
------------------

The new storage system for project data areas is now ready to be used. Starting now, projects can request to migrate, using the below migration process.

We will switch all project data areas to the 'new' storage on **14th May 2024**. If you have not migrated your data before then, your data area will be the new storage system (and your old data available read-only under /migration). 


How long will my data be available on the old storage after migration?
----------------------------------------------------------------------

The old storage system will be **decommissioned** on **1st August 2024**. After this date, the old data will no longer be available, and we will *not* be able to retrieve it.  
